---
title: "data-cleaning"
author: "Baron Curtin"
date: "March 22, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
lapply(c("tidyverse", "magrittr", "RCurl", "XML", "kableExtra"), require, character.only=TRUE)
```

## Load Dataset
```{r load-data}
plainText <- getURL("https://raw.githubusercontent.com/baroncurtin2/data607-project3/master/data/cleanjobfiles.txt") %>%
  read_lines %>%
  paste(collapse = " ")

searchTerms <- getURL("https://raw.githubusercontent.com/baroncurtin2/data607-project3/master/data/searchterms.csv") %>%
  read_csv(col_names = FALSE, trim_ws = TRUE) %>%
  # convert to vector
  pull(1)

# helper function
returnZero <- function(x) 0

# create empty data frame
termsDf <- data.frame(matrix(nrow = 300, ncol = length(searchTerms)))
# give dataframe column names
colnames(termsDf) <- searchTerms
# mutate rows so they are all the column name
termsDf %<>%
  mutate_all(funs(returnZero(.)))
```

## Convert Massive Text String to Vector to Data Frame
  * The objective here is to attempt to create a "regular" dataset
  * Key characteristics will be one job posting per row

```{r convert}
jobPostings <- plainText %>%
  # extract all instances of span tag
  str_extract_all('(<span id=(.*?)>)(.*?)(</span>)') %>%
  # unnest the list
  unlist %>%
  # convert to data frame
  data_frame(jobPost = .)
```


## Tidy the Data
  * The key objective here is to break up the singular column into multiple columns
```{r tidy}
# helper function

separateCols <- jobPostings %>%
  # separate jobPost column into the "summary" and "requirements" of role
  separate(col=jobPost, into=c("briefing", "requirements"), sep="<ul>|<li>", extra = "merge", fill="right") %>%
  # convert to lowercase
  mutate_at(vars(briefing:requirements), funs(str_to_lower))

# add search term columns
separateCols <- cbind(separateCols, termsDf)  

# dataset for non-NA
nonNA <- separateCols %>%
  # filter for non-NA
  filter(!is.na(requirements))
  #mutate_at(vars(ruby:visualization), funs())

# get counts of keywords
for(i in 3:ncol(nonNA)) {
  nonNA[, i] <- str_count(nonNA$requirements, str_c(c("([^[:alpha:]]", "[[:punct:][:blank:]]?", colnames(nonNA)[i], "[[:punct:][:blank:]]{1})"), collapse = ""))
}

# dataset for NA
dataNA <- separateCols %>%
  # filter for non-NA
  filter(is.na(requirements))

# get counts of keywords
for(i in 3:ncol(dataNA)) {
  dataNA[, i] <- str_count(dataNA$briefing, str_c(c("([^[:alpha:]]", "[[:punct:][:blank:]]?", colnames(dataNA)[i], "[[:punct:][:blank:]]{1})"), collapse = ""))
}

# supplemental cleaning dataNA
dataNA %<>%
  mutate(sql = str_count(requirements, "([[:punct:][:blank:]]?(sql)[[:punct:][:blank:]]{1})")) %>%
  mutate(machine_learning = str_count(requirements, "([[:punct:][:blank:]]?(machine learning)[[:punct:][:blank:]]{1})"))
 
# # supplemental cleaning nonNA
nonNA %<>%
  mutate(sql = str_count(briefing, "([[:punct:][:blank:]]?(sql)[[:punct:][:blank:]]{1})")) %>%
  mutate(machine_learning = str_count(briefing, "([[:punct:][:blank:]]?(machine learning)[[:punct:][:blank:]]{1})"))

# bind rows of NA and nonNA
jobPostings <- bind_rows(nonNA, dataNA) %>%
  select(-52)

# create keyword summary
keywordSummary <- jobPostings %>%
  select(-(1:2)) %>%
  gather(keyword, kw_count) %>%
  group_by(keyword) %>%
  summarise(kw_count = sum(kw_count, na.rm = TRUE))
```
  
  
  
## Generate CSV Files
```{r write-csv}
# show table
knitr::kable(head(jobPostings, 10), "html") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))

# write csv
write_csv(jobPostings, "./data/jobpostings.csv")
write_csv(keywordSummary, "./data/keywordsummary.csv")
```

